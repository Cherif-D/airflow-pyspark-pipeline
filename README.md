# Airflow Ã— PySpark â€” Pipeline de traitement de logs

## ğŸ“Œ Description

Ce projet met en place un pipeline automatisÃ© avec **Apache Airflow** et **PySpark** pour traiter des fichiers de logs. Le pipeline :

* Lit un fichier brut non triÃ© au format `timestamp @url`
* Extrait le **Breakdown\_Type** (nombre aprÃ¨s `slab/`)
* Extrait le **Breakdown\_Level** (nombre aprÃ¨s `256`)
* Trie par timestamp et, en cas d'Ã©galitÃ©, par Breakdown\_Type
* Sauvegarde les donnÃ©es propres dans `output/` et les lignes invalides dans `bad_lines/`

## ğŸ“‚ Structure du projet

```
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ breakdown_dag.py          # DÃ©finition du DAG Airflow
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ breakdown_job.py          # Script PySpark pour le traitement
â”œâ”€â”€ data/                         # DonnÃ©es d'entrÃ©e (logs bruts)
â”œâ”€â”€ output/                       # RÃ©sultats CSV gÃ©nÃ©rÃ©s
â”œâ”€â”€ bad_lines/                    # Lignes invalides
â”œâ”€â”€ docker-compose.yml            # Configuration des conteneurs Airflow
â”œâ”€â”€ Dockerfile                    # Image personnalisÃ©e (Java 17 + PySpark)
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â””â”€â”€ wheels/                       # Fichiers wheel pour installation offline
```

## ğŸ“¦ RÃ´le des fichiers **wheels**

Les **fichiers wheel (.whl)** sont des paquets Python prÃ©compilÃ©s. Dans ce projet, ils permettent :

* **Installation offline** : PySpark et ses dÃ©pendances peuvent Ãªtre installÃ©s mÃªme sans connexion Internet, ce qui est utile dans un environnement restreint.
* **Gain de temps** : lâ€™installation depuis des wheels est plus rapide que depuis le code source.
* **ReproductibilitÃ©** : on utilise exactement la mÃªme version des bibliothÃ¨ques, Ã©vitant des problÃ¨mes de compatibilitÃ©.

Ici, le Dockerfile copie ces wheels dans le conteneur et les installe localement :

```dockerfile
COPY wheels/ /wheels/
RUN pip install --no-cache-dir /wheels/py4j-*.whl && \
    pip install --no-cache-dir /wheels/pyspark-*-py2.py3-none-any.whl
```

Cela garantit que le pipeline fonctionne mÃªme dans un environnement sans accÃ¨s Internet.

## âš™ï¸ PrÃ©requis

* **Docker** & **Docker Compose**
* Port **8089** libre pour lâ€™interface Airflow
* Python 3.11 (si exÃ©cution locale des scripts)

## ğŸš€ Installation & Lancement

1. Cloner le dÃ©pÃ´t :

```bash
git clone https://github.com/Cherif-D/airflow-pyspark-pipeline.git
cd airflow-pyspark-pipeline
```

2. Initialiser Airflow :

```bash
docker compose up airflow-init
```

3. DÃ©marrer les services :

```bash
docker compose up -d
```

4. AccÃ©der Ã  lâ€™interface : [http://localhost:8089](http://localhost:8089)

## ğŸ“œ Utilisation du DAG

* **Nom du DAG** : `breakdown_pipeline`
* Ã‰tapes :

  1. `validate_input` â†’ VÃ©rifie la prÃ©sence du fichier source
  2. `fix_pyspark_perms` â†’ CrÃ©e et autorise les rÃ©pertoires nÃ©cessaires
  3. `Read` â†’ Lecture brute & identification des lignes invalides
  4. `SortData` â†’ Tri des donnÃ©es
  5. `Compute` â†’ Extraction des valeurs Breakdown et calculs
  6. `SaveResult` â†’ Sauvegarde des CSV

DÃ©clenchement manuel : via UI ou :

```bash
airflow dags trigger breakdown_pipeline
```

## ğŸ›  Commandes utiles

* Pause du DAG :

```bash
airflow dags pause breakdown_pipeline
```

* Reprise :

```bash
airflow dags unpause breakdown_pipeline
```

* Consultation des DAGs :

```bash
airflow dags list
```

## ğŸ“Œ Notes

* Aucun paramÃ©trage RAM/CPU nâ€™est dÃ©fini dans le dÃ©pÃ´t â†’ ajuster si besoin dans `docker-compose.yml` ou le code Spark.
* Les wheels sont essentiels si le dÃ©ploiement se fait dans un environnement sans accÃ¨s Internet.
